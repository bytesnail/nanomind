# nanomind åˆ†ç±»çŸ¥è¯†åº“

| å±æ€§ | å€¼ |
|------|-----|
| **è¦†ç›–èŒƒå›´** | `94eeacd` ~ `7731e66` (89 commits) |
| **æœ€åæ›´æ–°** | `7731e66` @ 2026-02-16 |

> æ›´æ–°æ—¶åŒæ­¥ä¿®æ”¹ commit hash å’Œæ—¥æœŸã€‚è§¦å‘æ¡ä»¶ï¼šæ–°å¢ commit â‰¥ 3 æˆ–æœ‰é‡å¤§å˜æ›´ã€‚


---

## å¢é‡æ›´æ–°æŒ‡å—(ä¸¥ç¦ä¿®æ”¹æˆ–åˆ é™¤æœ¬ç« èŠ‚)

**æ›´æ–°åŸåˆ™**ï¼š
- **æ–°çŸ¥è¯†** â†’ åœ¨å¯¹åº”ç« èŠ‚è¿½åŠ 
- **å·²æœ‰çŸ¥è¯†å˜æ›´** â†’ ç›´æ¥ä¿®æ”¹åŸå†…å®¹
- **è¿‡æ—¶/é”™è¯¯/ä¸éœ€è¦** â†’ ç›´æ¥åˆ é™¤
- **ä¿æŒç®€æ´** â†’ åˆå¹¶é‡å¤ï¼Œé¿å…å†—ä½™ï¼›ä»£ç ç¤ºä¾‹åªä¿ç•™æœ€ç›¸å…³æœ€é‡è¦çš„æœ€çŸ­ä»£ç ç‰‡æ®µï¼Œä¸¥ç¦æ•´æ®µæ‘˜æŠ„å¤åˆ¶å®Œæ•´ä»£ç å®ç°
- **å…ƒä¿¡æ¯** â†’ å…ƒä¿¡æ¯(æœ¬æ–‡ä»¶å¼€å¤´çš„è¦†ç›–èŒƒå›´å’Œæœ€åæ›´æ–°æ—¶é—´ç­‰ä¿¡æ¯)æ ¼å¼ä¸¥æ ¼ä¿æŒä¸å˜ï¼Œåªæ›´æ–°å…ƒä¿¡æ¯å†…å®¹
- **åˆ†æ³¢æ¬¡åˆ†æä¸ç”¨æˆ·ä¸»å¯¼** â†’ å…ˆå°†å¾…åˆ†æ commit æŒ‰ç›¸å…³æ€§å’Œæ•°é‡æ‹†åˆ†ä¸ºè¿è´¯çš„åˆ†ææ³¢æ¬¡ï¼›ä¸ä¸»åŠ¨æ›´æ–°æœ¬æ–‡ä»¶ï¼Œåªè¾“å‡ºè¯¦ç»†è®¡åˆ’ä¹¦ï¼Œç”±ç”¨æˆ·ä¸»å¯¼æ‰§è¡Œ

---


## ä¸€ã€Python è¯­æ³•çŸ¥è¯†

### 1.1 ç±»å‹æ³¨è§£

```python
# Union (Python 3.10+)
def find_bucket(score: float) -> BucketConfig | None: ...  # æ¨è
# ç­‰ä»·: Optional[BucketConfig]

# Final ä¸å¯å˜ (PEP 591)
EPSILON: Final = 1e-6

# Literal å­—é¢é‡ (PEP 586)
CompressionType = Literal["snappy", "gzip", "brotli", "lz4", "zstd"]

# TypeAlias (Python 3.10+)
DocHash: TypeAlias = tuple[int, str, Path, int]

# Forward Reference é¿å…å¾ªç¯å¯¼å…¥
self._bloom_filter: "ScalableBloomFilter | None" = None

# TYPE_CHECKING æ¡ä»¶å¯¼å…¥
if TYPE_CHECKING:
    from pybloom_live import ScalableBloomFilter
```

### 1.2 æ•°æ®ç±»

```python
@dataclass(frozen=True)
class BucketConfig:  # frozen=True ä½¿å®ä¾‹ä¸å¯å˜ï¼Œå¯ä½œå­—å…¸é”®
    name: str
    min_score: float
    max_score: float | None

@dataclass
class SamplingConfig:
    buckets: dict[str, int] = field(default_factory=dict)  # é¿å…å¯å˜é»˜è®¤å‚æ•°é™·é˜±

class DocHash(NamedTuple):  # è½»é‡æ•°æ®ç»“æ„
    hash_value: int
    doc_id: str
    file_path: Path
    row_index: int
```

### 1.3 æµ·è±¡è¿ç®—ç¬¦ä¸ç”Ÿæˆå™¨

```python
# æµ·è±¡è¿ç®—ç¬¦
if not (text := raw.get("text", "")): raise ValueError("text is missing")
if bucket := _BUCKET_MAP.get(name): return bucket
(path := log_dir / bucket).mkdir(parents=True, exist_ok=True)

# ç”Ÿæˆå™¨
def stream_file_rows(file_path: Path) -> Generator[tuple[int, str], None, None]:
    with pq.ParquetFile(file_path) as pf:
        for batch in pf.iter_batches(batch_size=10000):
            for row_idx, text in enumerate(batch.column("text").to_pylist()):
                yield row_idx, text
```

### 1.4 è£…é¥°å™¨ä¸å¸¸ç”¨æŠ€å·§

```python
# å»¶è¿ŸåŠ è½½
@property
def buckets(self) -> dict[str, Any]:
    return self._load("buckets")

# ç¼“å­˜
@lru_cache(maxsize=1024)
def get_file_row_count(file_path: Path) -> int:
    return pq.read_metadata(file_path).num_rows

# next + ç”Ÿæˆå™¨è¡¨è¾¾å¼ï¼šè¿”å›ç¬¬ä¸€ä¸ªåŒ¹é…é¡¹
return next((b for b in DEFAULT_BUCKETS if b.contains(score)), None)
```

### 1.5 å…¶ä»–è¯­æ³•

```python
# é“¾å¼æ¯”è¾ƒ
return self.min_score - EPSILON <= score < self.max_score

# ä¸‰å…ƒè¡¨è¾¾å¼
interval = f"[{self.min_score}, +âˆ)" if self.max_score is None else f"[{self.min_score}, {self.max_score})"

# æ•°å­—åˆ†éš”ç¬¦ (PEP 515)
bloom_capacity: int = 2_000_000_000
max_file_size: int = 512 * 1024 * 1024

# hashlib ç¡®å®šæ€§å“ˆå¸Œ
def compute_doc_hash(doc_id: str, seed: int) -> int:
    return int.from_bytes(
        hashlib.md5(f"{seed}_{doc_id}".encode(), usedforsecurity=False).digest()[:8], "big"
    )
```

---

## äºŒã€æ¡†æ¶çŸ¥è¯†

### 2.1 datatrove Pipeline

```python
# è‡ªå®šä¹‰ PipelineStep
class ScoreFilter(PipelineStep):
    name = "Score Filter"
    type = "ğŸ¯ - FILTER"
    def run(self, data, rank: int = 0, world_size: int = 1):
        for doc in data:
            if self._should_keep(doc): yield doc

# LocalPipelineExecutor
executor = LocalPipelineExecutor(
    pipeline=[ParquetReader(...), ScoreFilter(...), BucketPathWriter(...)],
    tasks=2500, workers=32, logging_dir=str(log_path),
)

# ParquetReader adapter
def fineweb_adapter(_reader, raw: dict, source: str, idx: int) -> dict:
    return {"text": raw.get("text", ""), "id": f"{source}#{idx}",
            "metadata": {"score": raw.get("score")}}

# Document ç»“æ„: text(å¿…éœ€), id(å¿…éœ€), metadata(dict), media(å¯é€‰)
```

### 2.2 PyArrow/Parquet

```python
# æµå¼è¯»å–ï¼ˆæ¨èï¼‰
with pq.ParquetFile(file_path) as pf:
    for batch in pf.iter_batches(batch_size=10000, columns=["text"]):
        process(batch)
# é¿å…: pq.read_table(file_path)  # å¯èƒ½ OOM

# å¿«é€Ÿè·å–è¡Œæ•°
row_count = pq.read_metadata(file_path).num_rows

# æµå¼å†™å…¥
schema = pa.schema([("text", pa.string()), ("id", pa.string())])
with pq.ParquetWriter(output_path, schema, compression="zstd") as writer:
    for batch in batches: writer.write_table(pa.table(batch))
```

### 2.3 concurrent.futures

```python
# ThreadPoolExecutor (IO å¯†é›†å‹)
with ThreadPoolExecutor(max_workers=32) as executor:
    future_to_file = {executor.submit(read_func, f): f for f in files}
    for future in as_completed(future_to_file):  # as_completed æŒ‰å®Œæˆé¡ºåºï¼Œæ›´å¿«
        file_path, result = future.result()
```

### 2.4 argparse / pytest / YAML

```python
# argparse
parser = argparse.ArgumentParser(formatter_class=argparse.RawDescriptionHelpFormatter)
parser.add_argument("--input", "-i", type=Path, default=Path("data/..."))
parser.add_argument("--bucket", choices=["2.8", "3.0", "3.5", "4.0"])

# pytest å‚æ•°åŒ–
@pytest.mark.parametrize("score,expected", [(3.5, "3.5"), (4.2, "4.0")])
def test_find_bucket(score, expected):
    assert find_bucket_for_score(score).name == expected

# YAML å®‰å…¨åŠ è½½
config = yaml.safe_load(f) or {}
```

---

## ä¸‰ã€é¡¹ç›®æ¶æ„çŸ¥è¯†

### 3.1 åŒ…ç»“æ„

```
src/data_processing/
â”œâ”€â”€ __init__.py              # å…¬å…± API å¯¼å‡º
â”œâ”€â”€ config_loader.py         # é…ç½®åŠ è½½
â”œâ”€â”€ bucket_config.py         # è¯„åˆ†æ¡¶é…ç½®ï¼ˆé€šç”¨ï¼‰
â”œâ”€â”€ score_filter.py          # è¯„åˆ†è¿‡æ»¤å™¨ï¼ˆé€šç”¨ï¼‰
â”œâ”€â”€ bucket_path_writer.py    # æ¡¶è·¯å¾„å†™å…¥å™¨ï¼ˆé€šç”¨ï¼‰
â””â”€â”€ fineweb_edu/             # FineWeb-Edu ä¸“ç”¨
    â”œâ”€â”€ __main__.py          # CLI: python -m src.data_processing.fineweb_edu
    â””â”€â”€ adapters.py          # æ•°æ®é€‚é…å™¨
```

### 3.2 é…ç½®åˆ†å±‚ä¸è®¾è®¡æ¨¡å¼

```
config/
â”œâ”€â”€ buckets.yaml      # ä¸šåŠ¡ï¼šè¯„åˆ†æ¡¶å®šä¹‰
â”œâ”€â”€ processing.yaml   # è¿è¡Œï¼šworkers, tasks, compression
â””â”€â”€ paths.yaml        # è·¯å¾„ï¼šè¾“å…¥/è¾“å‡ºç›®å½•
```

```python
# å»¶è¿ŸåŠ è½½æ¨¡å¼
_DEFAULT_BUCKETS: list[BucketConfig] | None = None
def get_all_bucket_configs() -> list[BucketConfig]:
    global _DEFAULT_BUCKETS
    if _DEFAULT_BUCKETS is None: _DEFAULT_BUCKETS = _load_buckets()
    return _DEFAULT_BUCKETS
```

### 3.3 Pipeline æ¶æ„

```
ParquetReader â†’ ScoreFilter â†’ BucketPathWriter
     â†“              â†“              â†“
  è¯»å–æ•°æ®      è¿‡æ»¤+é‡‡æ ·      å†™å…¥æ–‡ä»¶
```

| é˜¶æ®µ | æ“ä½œ | å†…å­˜ |
|------|------|------|
| é¢„è®¡ç®— | é‡‡æ ·ç´¢å¼• | O(target Ã— 16 bytes) |
| å¤„ç† | æµå¼ Pipeline | ä¸ç´¯ç§¯ |

---

## å››ã€ç›¸å…³å‚æ•°ä¸å‘½ä»¤

### 4.1 uv åŒ…ç®¡ç†

```bash
uv add <package> --no-sync              # æ·»åŠ ï¼ˆå¿…é¡»å¸¦ --no-syncï¼‰
uv pip compile pyproject.toml -o requirements.txt
uv pip install -r requirements.txt
```

### 4.2 æ€§èƒ½å‚æ•°

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `workers` | `min(16, cpu_count)` | å¹¶è¡Œè¿›ç¨‹æ•° |
| `tasks` | `workers` | æ•°æ®åˆ†ç‰‡æ•° |
| `max_file_size` | 10GB | è¾“å‡ºæ–‡ä»¶ä¸Šé™ |
| `batch_size` | 50,000 | æ‰¹æ¬¡å¤§å° |

```python
# å¹¶å‘å…¬å¼
max_workers = min(32, cpu_count * 2)  # IO å¯†é›†å‹
io_workers = max_workers * 2
```

### 4.3 æ•°æ®é›†è½¬æ¢

| æ•°æ®é›† | åŸå§‹èŒƒå›´ | è½¬æ¢ |
|--------|----------|------|
| FineWeb-EN | 0-5 | æ—  |
| FineWeb-ZH | 0-1 | `Ã— 5` |

---

## äº”ã€æœ€ä½³å®è·µ

- **ç±»å‹æ³¨è§£**ï¼šå‡½æ•°ç­¾åå¿…é¡»ç±»å‹æ³¨è§£
- **è·¯å¾„å¤„ç†**ï¼šä½¿ç”¨ `pathlib.Path`
- **èµ„æºç®¡ç†**ï¼šä½¿ç”¨ `with` è¯­å¥
- **é”™è¯¯å¤„ç†**ï¼šç²¾ç¡®å¼‚å¸¸æ•è·ï¼Œæ˜¾å¼æ£€æŸ¥æ›¿ä»£ assert
- **æ—¥å¿—**ï¼šä½¿ç”¨ `logging` è€Œé `print`
- **CLI**ï¼šæä¾›çŸ­/é•¿é€‰é¡¹ï¼Œè¿”å›æ ‡å‡†é€€å‡ºç 

---

## å…­ã€ç»éªŒæ•™è®­ âš ï¸

### 6.1 DOï¼ˆæ¨èåšæ³•ï¼‰

**ä¾èµ–ç®¡ç†**
- `uv add <package> --no-sync` â†’ `uv pip compile` â†’ `uv pip install -r`
- å¿…é¡»å¸¦ `--no-sync` é¿å… uv.lock

**é…ç½®ç®¡ç†**
- `yaml.safe_load()` è€Œé `yaml.load()`
- å»¶è¿ŸåŠ è½½é¿å…å¾ªç¯ä¾èµ–ï¼Œä¿ç•™åˆç†é»˜è®¤å€¼

**ä»£ç è´¨é‡**
- å‘½åå¸¸é‡æ›¿ä»£é­”æ³•æ•°å­—
- å¯å¤ç”¨é€»è¾‘æå–åˆ°æ¨¡å—
- å®šæœŸ `gc.collect()` é˜²æ­¢å†…å­˜ç¢ç‰‡

**æ•°æ®å¤„ç†**
- `encode("utf-8")` è·å–å­—ç¬¦ä¸²å®é™…å­—èŠ‚å¤§å°
- `iter_batches()` æµå¼è¯»å–å¤§æ–‡ä»¶
- å­˜å‚¨æ•´æ•°ç´¢å¼•è€Œéå®Œæ•´å¯¹è±¡

**å¹¶å‘å¤„ç†**
- `as_completed()` è·å–å·²å®Œæˆä»»åŠ¡
- IO å¯†é›†å‹ç”¨ `ThreadPoolExecutor`
- åœ¨ `with` è¯­å¥ä¸­ä½¿ç”¨ Executor

**æ¡†æ¶ä½¿ç”¨**
- PipelineStep è®¾ç½® `name` å’Œ `type` å±æ€§
- adapter å‡½æ•°æ‰©å±• Reader è¡Œä¸º

### 6.2 DO NOTï¼ˆä¸æ¨èåšæ³•ï¼‰

| ç±»åˆ« | ç¦æ­¢ |
|------|------|
| é…ç½® | æ¨¡å—é¡¶å±‚ç›´æ¥åŠ è½½ã€å‡è®¾å­—æ®µä¸€å®šå­˜åœ¨ |
| ä»£ç  | è£¸ `except:`ã€ç”Ÿäº§ä»£ç ç”¨ `assert`ã€`print` è¾“å‡ºæ—¥å¿— |
| æ•°æ® | å‡è®¾å›ºå®šå®½åº¦ç¼–ç ã€å…¨é‡åŠ è½½åå¤„ç†ã€å¾ªç¯å†… import |
| å¹¶å‘ | ä¸²è¡Œé€»è¾‘å¤„ç†å¹¶è¡Œåœºæ™¯ã€å †ä¸­å­˜å‚¨ Path ç­‰å¤§å¯¹è±¡ |
| æ¶æ„ | å¯å¤ç”¨é€»è¾‘ç•™åœ¨è„šæœ¬ã€æ•°æ®é›†é€»è¾‘æ”¾é€šç”¨æ¨¡å— |

### 6.3 NEVERï¼ˆç¦æ­¢åšæ³•ï¼‰

| ç±»åˆ« | ç¦æ­¢ |
|------|------|
| ä¾èµ– | `pip install`ã€`uv add` ä¸å¸¦ `--no-sync`ã€æäº¤ uv.lock |
| ç±»å‹ | `as any`ã€`@ts-ignore`ã€`@ts-expect-error` |
| æ•°æ® | è´Ÿæ•°ç´¢å¼•/ç©ºè·¯å¾„ç”Ÿæˆ IDã€é‡‡æ ·å¾ªç¯åˆ›å»ºå¤§é‡ä¸´æ—¶å¯¹è±¡ |
| æ¡†æ¶ | å¿½ç•¥ Datatrove ä»»åŠ¡æ£€æµ‹ã€Writer è¿”å›é None |
| å†…å­˜ | ä¸€æ¬¡æ€§åŠ è½½å¤§è§„æ¨¡æ•°æ®é›†ã€å…±äº« Datatrove æ—¥å¿—ç›®å½• |

---

## ä¸ƒã€å¸¸è§é—®é¢˜

### 7.1 æ–‡ä»¶å¤§å°ä¼°ç®—é”™è¯¯
- **é—®é¢˜**ï¼šè¾“å‡º 20-200MB è€Œé 1-2GB
- **åŸå› **ï¼š`len(text) * 2` ä¼°ç®— UTF-8
- **è§£å†³**ï¼š`len(text.encode("utf-8")) + 32`

### 7.2 Datatrove è·³è¿‡ä»»åŠ¡
- **é—®é¢˜**ï¼šåç»­æ•°æ®é›†è¢«è·³è¿‡
- **åŸå› **ï¼šå…±äº« `logging_dir`
- **è§£å†³**ï¼š`log_name = f"multi_bucket_{output_dir.name}"`

### 7.3 IndexFilter ä¸ç”Ÿæ•ˆ
- **é—®é¢˜**ï¼šæ— æ³•è¿‡æ»¤æ–‡æ¡£
- **åŸå› **ï¼šParquetReader é»˜è®¤ä¸è®¾ç½® `row_idx`
- **è§£å†³**ï¼šadapter æ·»åŠ  `metadata["row_idx"] = id_in_file`

### 7.4 å†…å­˜æ³„æ¼
- **é—®é¢˜**ï¼šé•¿æ—¶é—´è¿è¡Œå†…å­˜å¢é•¿
- **åŸå› **ï¼šglibc ptmalloc2 ç¢ç‰‡
- **è§£å†³**ï¼šå¯ç”¨ jemalloc
```python
if os.path.exists("/usr/lib/x86_64-linux-gnu/libjemalloc.so.2"):
    os.environ.setdefault("LD_PRELOAD", "/usr/lib/x86_64-linux-gnu/libjemalloc.so.2")
```

---

*æ–‡æ¡£ç‰ˆæœ¬: v1.1 | ç”Ÿæˆæ—¥æœŸ: 2026-02-16*
