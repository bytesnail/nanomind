# nanomind åˆ†ç±»çŸ¥è¯†åº“

| å±æ€§ | å€¼ |
|------|-----|
| **è¦†ç›–èŒƒå›´** | `94eeacd` ~ `7731e66` (89 commits) |
| **æœ€åæ›´æ–°** | `7731e66` @ 2026-02-16 |

> ğŸ’¡ **ä½¿ç”¨æ–¹å¼**ï¼š`æ ¹æ®docs/KNOWLEDGE_BASE.mdçš„å¢é‡æ›´æ–°æŒ‡å—ï¼Œåˆ†æ æ‰€æœ‰æœªåˆ†æcommit çš„çŸ¥è¯†åº“æ›´æ–°è®¡åˆ’` â†’ è¾“å‡ºè®¡åˆ’ä¹¦ â†’ `æ‰§è¡Œè®¡åˆ’` â†’ `åº”ç”¨æ›´æ–°`

---

## å¢é‡æ›´æ–°æŒ‡å—

> âš ï¸ æœ¬ç« èŠ‚æä¾›æ›´æ–°è§„åˆ™ï¼Œå¦‚éœ€ä¿®æ”¹æœ¬ç« èŠ‚è¯·å…ˆä¸ç”¨æˆ·ç¡®è®¤ã€‚

### æœ¯è¯­å®šä¹‰

| æœ¯è¯­ | å®šä¹‰ |
|------|------|
| **å…ƒä¿¡æ¯** | æ–‡æ¡£å¤´éƒ¨è¡¨æ ¼ï¼ˆè¦†ç›–èŒƒå›´ + æœ€åæ›´æ–°ï¼‰ï¼Œæ ¼å¼å›ºå®šï¼Œä»…æ›´æ–°å†…å®¹ |
| **æ–°çŸ¥è¯†** | ä»£ç åº“ä¸­é¦–æ¬¡å‡ºç°çš„æ¨¡å¼/æŠ€æœ¯/è§£å†³æ–¹æ¡ˆ |
| **çŸ¥è¯†å˜æ›´** | å·²è®°å½•çŸ¥è¯†çš„ç‰ˆæœ¬å‡çº§ã€æ›¿ä»£æ–¹æ¡ˆã€åºŸå¼ƒæˆ–ä¿®æ­£ |
| **é‡å¤§å˜æ›´** | æ»¡è¶³ä»¥ä¸‹ä»»ä¸€ï¼šæ–°å¢/åˆ é™¤æ¨¡å—ã€ä¿®æ”¹å…¬å¼€APIã€æ¶‰åŠå®‰å…¨æ¼æ´ã€å˜æ›´æ ¸å¿ƒä¾èµ– |

### è§¦å‘æ¡ä»¶

| æ¡ä»¶ | åŠ¨ä½œ |
|------|------|
| å«ä»»æ„é‡å¤§å˜æ›´ï¼ˆcommit æ•°ä¸é™ï¼‰ | èµ°ç®€åŒ–æµç¨‹ï¼šç›´æ¥è¾“å‡ºæ›´æ–°å†…å®¹ â†’ ç”¨æˆ·ç¡®è®¤ |
| å¾…åˆ†æ commit æ•° â‰¥ 3 | å¯åŠ¨æ›´æ–°åˆ†æï¼ˆèµ°å®Œæ•´æµç¨‹ï¼‰ |
| å…¶ä»–æƒ…å†µ | å¿½ç•¥ï¼Œä¸åšä»»ä½•æ›´æ–° |
### æ›´æ–°åŸåˆ™

| åœºæ™¯ | åŠ¨ä½œ |
|------|------|
| **æ–°çŸ¥è¯†** | å¯¹åº”ç« èŠ‚è¿½åŠ  |
| **çŸ¥è¯†å˜æ›´** | ç›´æ¥ä¿®æ”¹åŸå†…å®¹ |
| **è¿‡æ—¶/é”™è¯¯** | ç›´æ¥åˆ é™¤ |
| **é‡å¤å†…å®¹** | åˆå¹¶è‡³ä¸€å¤„ |

### ä»£ç ç¤ºä¾‹è§„åˆ™

- âœ… ä¿ç•™ï¼šâ‰¤10è¡Œï¼ˆæ ¸å¿ƒé€»è¾‘è¶…é™æ—¶ï¼Œæ‹†åˆ†ä¸ºæ‘˜è¦ + å¤šä¸ªå…³é”®ç‰‡æ®µï¼‰ï¼Œä¼˜å…ˆçº§ï¼šæ ¸å¿ƒé€»è¾‘ > ç®€æ´ > å®Œæ•´ä¸Šä¸‹æ–‡
- âŒ ç¦æ­¢ï¼šæ•´æ®µå¤åˆ¶å®Œæ•´å‡½æ•°/ç±»å®ç°
- ğŸ“ æ›¿ä»£ï¼šæ‘˜è¦ï¼ˆåŠŸèƒ½æè¿° + å…³é”® API/å‚æ•°ï¼‰+ æ–‡ä»¶è·¯å¾„:è¡Œå·

### ç« èŠ‚å½’å±

| çŸ¥è¯†ç±»å‹ | ç›®æ ‡ç« èŠ‚ |
|----------|----------|
| çº¯ Python è¯­æ³• | ä¸€ã€Python è¯­æ³•çŸ¥è¯† |
| æ¡†æ¶/åº“ API | äºŒã€æ¡†æ¶çŸ¥è¯† |
| é¡¹ç›®ç»“æ„/é…ç½® | ä¸‰ã€é¡¹ç›®æ¶æ„çŸ¥è¯† |
| å‚æ•°/å‘½ä»¤ | å››ã€ç›¸å…³å‚æ•°ä¸å‘½ä»¤ |
| æœ€ä½³å®è·µ | äº”ã€æœ€ä½³å®è·µ |
| è¸©å‘/ä¿®å¤ | å…­ã€ç»éªŒæ•™è®­ |
| FAQ | ä¸ƒã€å¸¸è§é—®é¢˜ |
| **è·¨ç« èŠ‚** | ä¸»ç« èŠ‚è®°å½•æ ¸å¿ƒï¼Œå…¶ä»–ç« èŠ‚ç”¨ â†’ å¼•ç”¨ |

### è®¡åˆ’ä¹¦æ ¼å¼

æ¯æ¬¡æ›´æ–°å‰è¾“å‡ºåˆ†ææ¡†æ¶ï¼Œè€Œéåˆ†æç»“æœï¼š

```markdown
## æ›´æ–°è®¡åˆ’ [æ³¢æ¬¡ N/M]

**å¾…åˆ†æ Commits**: `abc123` `def456` ...ï¼ˆå…± X ä¸ªï¼‰
**åˆ†ç»„ç­–ç•¥**: æŒ‰ç« èŠ‚/ç›¸å…³æ€§æ‹†åˆ†ä¸º M ä¸ªæ³¢æ¬¡

### æœ¬æ³¢æ¬¡åˆ†æé¡¹

| åˆ†æé¡¹ | è¾“å‡ºæ ¼å¼ | è¯´æ˜ |
|--------|----------|------|
| commit å½’å±ç« èŠ‚ | ç« èŠ‚åç§°åˆ—è¡¨ | æ¯ä¸ª commit å½±å“å“ªäº›ç« èŠ‚ |
| å˜æ›´ç±»å‹åˆ¤å®š | æ–°å¢/ä¿®æ”¹/åˆ é™¤ | å¯¹ç°æœ‰çŸ¥è¯†çš„å½±å“ç±»å‹ |
| ä»£ç ç¤ºä¾‹æå– | æ–‡ä»¶è·¯å¾„:è¡Œå· | éœ€æå–çš„ä»£ç ç‰‡æ®µä½ç½® |
| å†—ä½™æ£€æµ‹ | é‡å¤æ¡ç›®åˆ—è¡¨ | æ–¹æ³•ï¼šå…³é”®è¯æœç´¢ + ç›¸åŒæ–‡ä»¶è·¯å¾„ + æŠ€æœ¯ç‚¹é‡åˆ |
```

> ğŸ’¡ åˆ†æ‰¹æ¬¡åŸå› ï¼šå¾…åˆ†æå†…å®¹å¯èƒ½è¾ƒå¤šï¼Œä¸€æ¬¡æ€§åˆ†æéš¾ä»¥ä¿è¯å‡†ç¡®æ€§å’Œå®Œæ•´æ€§ã€‚
### æ‰§è¡Œæµç¨‹

```
1. åˆ—å‡ºå¾…åˆ†æ commits â†’ æŒ‰æ•°é‡/ç›¸å…³æ€§åˆ†ç»„
2. è¾“å‡ºè®¡åˆ’ä¹¦ï¼ˆåˆ†ææ¡†æ¶ï¼‰â†’ ç­‰å¾…ç”¨æˆ·ç¡®è®¤
3. ç”¨æˆ·æ‰¹å‡† â†’ æŒ‰æ³¢æ¬¡æ‰§è¡Œåˆ†æ
4. æ¯æ³¢æ¬¡åˆ†æå®Œæˆ â†’ è¾“å‡ºè¯¥æ‰¹æ¬¡æ›´æ–°å†…å®¹ â†’ ç”¨æˆ·ç¡®è®¤
5. å…¨éƒ¨æ³¢æ¬¡å®Œæˆ â†’ æ›´æ–°å…ƒä¿¡æ¯
6. ç”¨æˆ·æ‹’ç»/è¶…æ—¶ï¼ˆ>24hï¼‰â†’ ç»“æŸæœ¬æ¬¡æ›´æ–°ï¼Œä¸åšä»»ä½•ä¿®æ”¹
```

### ä¼˜å…ˆçº§

```
ç”¨æˆ·æ˜ç¡®æŒ‡ä»¤ > ç®€æ´æ€§ > å®Œæ•´æ€§
```

**å†²çªåœºæ™¯å¤„ç†**ï¼š
- ç®€æ´æ€§ vs å®Œæ•´æ€§å†²çª â†’ ä¼˜å…ˆæ‘˜è¦ + æ–‡ä»¶è·¯å¾„:è¡Œå·å¼•ç”¨
- ä¸ç¡®å®šæ—¶ â†’ è¯¢é—®ç”¨æˆ·

### åº•éƒ¨ç‰ˆæœ¬å·

æ–‡æ¡£åº•éƒ¨ `*æ–‡æ¡£ç‰ˆæœ¬: vX.Y*` éšæ¯æ¬¡å†…å®¹æ›´æ–°é€’å¢ï¼š
- å°ä¿®ï¼ˆé”™åˆ«å­—ã€æ ¼å¼ï¼‰ï¼šX.Y â†’ X.Y+1
- å†…å®¹å˜æ›´ï¼ˆæ–°å¢/ä¿®æ”¹/åˆ é™¤ï¼‰ï¼šX.Y â†’ X+1.0
---


## ä¸€ã€Python è¯­æ³•çŸ¥è¯†

### 1.1 ç±»å‹æ³¨è§£

```python
# Union (Python 3.10+)
def find_bucket(score: float) -> BucketConfig | None: ...  # æ¨è
# ç­‰ä»·: Optional[BucketConfig]

# Final ä¸å¯å˜ (PEP 591)
EPSILON: Final = 1e-6

# Literal å­—é¢é‡ (PEP 586)
CompressionType = Literal["snappy", "gzip", "brotli", "lz4", "zstd"]

# TypeAlias (Python 3.10+)
DocHash: TypeAlias = tuple[int, str, Path, int]

# Forward Reference é¿å…å¾ªç¯å¯¼å…¥
self._bloom_filter: "ScalableBloomFilter | None" = None

# TYPE_CHECKING æ¡ä»¶å¯¼å…¥
if TYPE_CHECKING:
    from pybloom_live import ScalableBloomFilter
```

### 1.2 æ•°æ®ç±»

```python
@dataclass(frozen=True)
class BucketConfig:  # frozen=True ä½¿å®ä¾‹ä¸å¯å˜ï¼Œå¯ä½œå­—å…¸é”®
    name: str
    min_score: float
    max_score: float | None

@dataclass
class SamplingConfig:
    buckets: dict[str, int] = field(default_factory=dict)  # é¿å…å¯å˜é»˜è®¤å‚æ•°é™·é˜±

class DocHash(NamedTuple):  # è½»é‡æ•°æ®ç»“æ„
    hash_value: int
    doc_id: str
    file_path: Path
    row_index: int
```

### 1.3 æµ·è±¡è¿ç®—ç¬¦ä¸ç”Ÿæˆå™¨

```python
# æµ·è±¡è¿ç®—ç¬¦
if not (text := raw.get("text", "")): raise ValueError("text is missing")
if bucket := _BUCKET_MAP.get(name): return bucket
(path := log_dir / bucket).mkdir(parents=True, exist_ok=True)

# ç”Ÿæˆå™¨
def stream_file_rows(file_path: Path) -> Generator[tuple[int, str], None, None]:
    with pq.ParquetFile(file_path) as pf:
        for batch in pf.iter_batches(batch_size=10000):
            for row_idx, text in enumerate(batch.column("text").to_pylist()):
                yield row_idx, text
```

### 1.4 è£…é¥°å™¨ä¸å¸¸ç”¨æŠ€å·§

```python
# å»¶è¿ŸåŠ è½½
@property
def buckets(self) -> dict[str, Any]:
    return self._load("buckets")

# ç¼“å­˜
@lru_cache(maxsize=1024)
def get_file_row_count(file_path: Path) -> int:
    return pq.read_metadata(file_path).num_rows

# next + ç”Ÿæˆå™¨è¡¨è¾¾å¼ï¼šè¿”å›ç¬¬ä¸€ä¸ªåŒ¹é…é¡¹
return next((b for b in DEFAULT_BUCKETS if b.contains(score)), None)
```

### 1.5 å…¶ä»–è¯­æ³•

```python
# é“¾å¼æ¯”è¾ƒ
return self.min_score - EPSILON <= score < self.max_score

# ä¸‰å…ƒè¡¨è¾¾å¼
interval = f"[{self.min_score}, +âˆ)" if self.max_score is None else f"[{self.min_score}, {self.max_score})"

# æ•°å­—åˆ†éš”ç¬¦ (PEP 515)
bloom_capacity: int = 2_000_000_000
max_file_size: int = 512 * 1024 * 1024

# hashlib ç¡®å®šæ€§å“ˆå¸Œ
def compute_doc_hash(doc_id: str, seed: int) -> int:
    return int.from_bytes(
        hashlib.md5(f"{seed}_{doc_id}".encode(), usedforsecurity=False).digest()[:8], "big"
    )
```

---

## äºŒã€æ¡†æ¶çŸ¥è¯†

### 2.1 datatrove Pipeline

```python
# è‡ªå®šä¹‰ PipelineStep
class ScoreFilter(PipelineStep):
    name = "Score Filter"
    type = "ğŸ¯ - FILTER"
    def run(self, data, rank: int = 0, world_size: int = 1):
        for doc in data:
            if self._should_keep(doc): yield doc

# LocalPipelineExecutor
executor = LocalPipelineExecutor(
    pipeline=[ParquetReader(...), ScoreFilter(...), BucketPathWriter(...)],
    tasks=2500, workers=32, logging_dir=str(log_path),
)

# ParquetReader adapter
def fineweb_adapter(_reader, raw: dict, source: str, idx: int) -> dict:
    return {"text": raw.get("text", ""), "id": f"{source}#{idx}",
            "metadata": {"score": raw.get("score")}}

# Document ç»“æ„: text(å¿…éœ€), id(å¿…éœ€), metadata(dict), media(å¯é€‰)
```

### 2.2 PyArrow/Parquet

```python
# æµå¼è¯»å–ï¼ˆæ¨èï¼‰
with pq.ParquetFile(file_path) as pf:
    for batch in pf.iter_batches(batch_size=10000, columns=["text"]):
        process(batch)
# é¿å…: pq.read_table(file_path)  # å¯èƒ½ OOM

# å¿«é€Ÿè·å–è¡Œæ•°
row_count = pq.read_metadata(file_path).num_rows

# æµå¼å†™å…¥
schema = pa.schema([("text", pa.string()), ("id", pa.string())])
with pq.ParquetWriter(output_path, schema, compression="zstd") as writer:
    for batch in batches: writer.write_table(pa.table(batch))
```

### 2.3 concurrent.futures

```python
# ThreadPoolExecutor (IO å¯†é›†å‹)
with ThreadPoolExecutor(max_workers=32) as executor:
    future_to_file = {executor.submit(read_func, f): f for f in files}
    for future in as_completed(future_to_file):  # as_completed æŒ‰å®Œæˆé¡ºåºï¼Œæ›´å¿«
        file_path, result = future.result()
```

### 2.4 argparse / pytest / YAML

```python
# argparse
parser = argparse.ArgumentParser(formatter_class=argparse.RawDescriptionHelpFormatter)
parser.add_argument("--input", "-i", type=Path, default=Path("data/..."))
parser.add_argument("--bucket", choices=["2.8", "3.0", "3.5", "4.0"])

# pytest å‚æ•°åŒ–
@pytest.mark.parametrize("score,expected", [(3.5, "3.5"), (4.2, "4.0")])
def test_find_bucket(score, expected):
    assert find_bucket_for_score(score).name == expected

# YAML å®‰å…¨åŠ è½½
config = yaml.safe_load(f) or {}
```

---

## ä¸‰ã€é¡¹ç›®æ¶æ„çŸ¥è¯†

### 3.1 åŒ…ç»“æ„

```
src/data_processing/
â”œâ”€â”€ __init__.py              # å…¬å…± API å¯¼å‡º
â”œâ”€â”€ config_loader.py         # é…ç½®åŠ è½½
â”œâ”€â”€ bucket_config.py         # è¯„åˆ†æ¡¶é…ç½®ï¼ˆé€šç”¨ï¼‰
â”œâ”€â”€ score_filter.py          # è¯„åˆ†è¿‡æ»¤å™¨ï¼ˆé€šç”¨ï¼‰
â”œâ”€â”€ bucket_path_writer.py    # æ¡¶è·¯å¾„å†™å…¥å™¨ï¼ˆé€šç”¨ï¼‰
â””â”€â”€ fineweb_edu/             # FineWeb-Edu ä¸“ç”¨
    â”œâ”€â”€ __main__.py          # CLI: python -m src.data_processing.fineweb_edu
    â””â”€â”€ adapters.py          # æ•°æ®é€‚é…å™¨
```

### 3.2 é…ç½®åˆ†å±‚ä¸è®¾è®¡æ¨¡å¼

```
config/
â”œâ”€â”€ buckets.yaml      # ä¸šåŠ¡ï¼šè¯„åˆ†æ¡¶å®šä¹‰
â”œâ”€â”€ processing.yaml   # è¿è¡Œï¼šworkers, tasks, compression
â””â”€â”€ paths.yaml        # è·¯å¾„ï¼šè¾“å…¥/è¾“å‡ºç›®å½•
```

```python
# å»¶è¿ŸåŠ è½½æ¨¡å¼
_DEFAULT_BUCKETS: list[BucketConfig] | None = None
def get_all_bucket_configs() -> list[BucketConfig]:
    global _DEFAULT_BUCKETS
    if _DEFAULT_BUCKETS is None: _DEFAULT_BUCKETS = _load_buckets()
    return _DEFAULT_BUCKETS
```

### 3.3 Pipeline æ¶æ„

```
ParquetReader â†’ ScoreFilter â†’ BucketPathWriter
     â†“              â†“              â†“
  è¯»å–æ•°æ®      è¿‡æ»¤+é‡‡æ ·      å†™å…¥æ–‡ä»¶
```

| é˜¶æ®µ | æ“ä½œ | å†…å­˜ |
|------|------|------|
| é¢„è®¡ç®— | é‡‡æ ·ç´¢å¼• | O(target Ã— 16 bytes) |
| å¤„ç† | æµå¼ Pipeline | ä¸ç´¯ç§¯ |

---

## å››ã€ç›¸å…³å‚æ•°ä¸å‘½ä»¤

### 4.1 uv åŒ…ç®¡ç†

```bash
uv add <package> --no-sync              # æ·»åŠ ï¼ˆå¿…é¡»å¸¦ --no-syncï¼‰
uv pip compile pyproject.toml -o requirements.txt
uv pip install -r requirements.txt
```

### 4.2 æ€§èƒ½å‚æ•°

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `workers` | `min(16, cpu_count)` | å¹¶è¡Œè¿›ç¨‹æ•° |
| `tasks` | `workers` | æ•°æ®åˆ†ç‰‡æ•° |
| `max_file_size` | 10GB | è¾“å‡ºæ–‡ä»¶ä¸Šé™ |
| `batch_size` | 50,000 | æ‰¹æ¬¡å¤§å° |

```python
# å¹¶å‘å…¬å¼
max_workers = min(32, cpu_count * 2)  # IO å¯†é›†å‹
io_workers = max_workers * 2
```

### 4.3 æ•°æ®é›†è½¬æ¢

| æ•°æ®é›† | åŸå§‹èŒƒå›´ | è½¬æ¢ |
|--------|----------|------|
| FineWeb-EN | 0-5 | æ—  |
| FineWeb-ZH | 0-1 | `Ã— 5` |

---

## äº”ã€æœ€ä½³å®è·µ

- **ç±»å‹æ³¨è§£**ï¼šå‡½æ•°ç­¾åå¿…é¡»ç±»å‹æ³¨è§£
- **è·¯å¾„å¤„ç†**ï¼šä½¿ç”¨ `pathlib.Path`
- **èµ„æºç®¡ç†**ï¼šä½¿ç”¨ `with` è¯­å¥
- **é”™è¯¯å¤„ç†**ï¼šç²¾ç¡®å¼‚å¸¸æ•è·ï¼Œæ˜¾å¼æ£€æŸ¥æ›¿ä»£ assert
- **æ—¥å¿—**ï¼šä½¿ç”¨ `logging` è€Œé `print`
- **CLI**ï¼šæä¾›çŸ­/é•¿é€‰é¡¹ï¼Œè¿”å›æ ‡å‡†é€€å‡ºç 

---

## å…­ã€ç»éªŒæ•™è®­ âš ï¸

### 6.1 DOï¼ˆæ¨èåšæ³•ï¼‰

**ä¾èµ–ç®¡ç†**
- `uv add <package> --no-sync` â†’ `uv pip compile` â†’ `uv pip install -r`
- å¿…é¡»å¸¦ `--no-sync` é¿å… uv.lock

**é…ç½®ç®¡ç†**
- `yaml.safe_load()` è€Œé `yaml.load()`
- å»¶è¿ŸåŠ è½½é¿å…å¾ªç¯ä¾èµ–ï¼Œä¿ç•™åˆç†é»˜è®¤å€¼

**ä»£ç è´¨é‡**
- å‘½åå¸¸é‡æ›¿ä»£é­”æ³•æ•°å­—
- å¯å¤ç”¨é€»è¾‘æå–åˆ°æ¨¡å—
- å®šæœŸ `gc.collect()` é˜²æ­¢å†…å­˜ç¢ç‰‡

**æ•°æ®å¤„ç†**
- `encode("utf-8")` è·å–å­—ç¬¦ä¸²å®é™…å­—èŠ‚å¤§å°
- `iter_batches()` æµå¼è¯»å–å¤§æ–‡ä»¶
- å­˜å‚¨æ•´æ•°ç´¢å¼•è€Œéå®Œæ•´å¯¹è±¡

**å¹¶å‘å¤„ç†**
- `as_completed()` è·å–å·²å®Œæˆä»»åŠ¡
- IO å¯†é›†å‹ç”¨ `ThreadPoolExecutor`
- åœ¨ `with` è¯­å¥ä¸­ä½¿ç”¨ Executor

**æ¡†æ¶ä½¿ç”¨**
- PipelineStep è®¾ç½® `name` å’Œ `type` å±æ€§
- adapter å‡½æ•°æ‰©å±• Reader è¡Œä¸º

### 6.2 DO NOTï¼ˆä¸æ¨èåšæ³•ï¼‰

| ç±»åˆ« | ç¦æ­¢ |
|------|------|
| é…ç½® | æ¨¡å—é¡¶å±‚ç›´æ¥åŠ è½½ã€å‡è®¾å­—æ®µä¸€å®šå­˜åœ¨ |
| ä»£ç  | è£¸ `except:`ã€ç”Ÿäº§ä»£ç ç”¨ `assert`ã€`print` è¾“å‡ºæ—¥å¿— |
| æ•°æ® | å‡è®¾å›ºå®šå®½åº¦ç¼–ç ã€å…¨é‡åŠ è½½åå¤„ç†ã€å¾ªç¯å†… import |
| å¹¶å‘ | ä¸²è¡Œé€»è¾‘å¤„ç†å¹¶è¡Œåœºæ™¯ã€å †ä¸­å­˜å‚¨ Path ç­‰å¤§å¯¹è±¡ |
| æ¶æ„ | å¯å¤ç”¨é€»è¾‘ç•™åœ¨è„šæœ¬ã€æ•°æ®é›†é€»è¾‘æ”¾é€šç”¨æ¨¡å— |

### 6.3 NEVERï¼ˆç¦æ­¢åšæ³•ï¼‰

| ç±»åˆ« | ç¦æ­¢ |
|------|------|
| ä¾èµ– | `pip install`ã€`uv add` ä¸å¸¦ `--no-sync`ã€æäº¤ uv.lock |
| ç±»å‹ | `as any`ã€`@ts-ignore`ã€`@ts-expect-error` |
| æ•°æ® | è´Ÿæ•°ç´¢å¼•/ç©ºè·¯å¾„ç”Ÿæˆ IDã€é‡‡æ ·å¾ªç¯åˆ›å»ºå¤§é‡ä¸´æ—¶å¯¹è±¡ |
| æ¡†æ¶ | å¿½ç•¥ Datatrove ä»»åŠ¡æ£€æµ‹ã€Writer è¿”å›é None |
| å†…å­˜ | ä¸€æ¬¡æ€§åŠ è½½å¤§è§„æ¨¡æ•°æ®é›†ã€å…±äº« Datatrove æ—¥å¿—ç›®å½• |

---

## ä¸ƒã€å¸¸è§é—®é¢˜

### 7.1 æ–‡ä»¶å¤§å°ä¼°ç®—é”™è¯¯
- **é—®é¢˜**ï¼šè¾“å‡º 20-200MB è€Œé 1-2GB
- **åŸå› **ï¼š`len(text) * 2` ä¼°ç®— UTF-8
- **è§£å†³**ï¼š`len(text.encode("utf-8")) + 32`

### 7.2 Datatrove è·³è¿‡ä»»åŠ¡
- **é—®é¢˜**ï¼šåç»­æ•°æ®é›†è¢«è·³è¿‡
- **åŸå› **ï¼šå…±äº« `logging_dir`
- **è§£å†³**ï¼š`log_name = f"multi_bucket_{output_dir.name}"`

### 7.3 IndexFilter ä¸ç”Ÿæ•ˆ
- **é—®é¢˜**ï¼šæ— æ³•è¿‡æ»¤æ–‡æ¡£
- **åŸå› **ï¼šParquetReader é»˜è®¤ä¸è®¾ç½® `row_idx`
- **è§£å†³**ï¼šadapter æ·»åŠ  `metadata["row_idx"] = id_in_file`

### 7.4 å†…å­˜æ³„æ¼
- **é—®é¢˜**ï¼šé•¿æ—¶é—´è¿è¡Œå†…å­˜å¢é•¿
- **åŸå› **ï¼šglibc ptmalloc2 ç¢ç‰‡
- **è§£å†³**ï¼šå¯ç”¨ jemalloc
```python
if os.path.exists("/usr/lib/x86_64-linux-gnu/libjemalloc.so.2"):
    os.environ.setdefault("LD_PRELOAD", "/usr/lib/x86_64-linux-gnu/libjemalloc.so.2")
```

---

*æ–‡æ¡£ç‰ˆæœ¬: v2.0 | ç”Ÿæˆæ—¥æœŸ: 2026-03-01*
